{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Learning PySpark Basics\n",
    "## The following will be covered:\n",
    "* PySpark Dataframe\n",
    "* Reading the Dataset\n",
    "* Checking the Datatypes of the Column(Schema)\n",
    "* Selecting Columns and Indexing\n",
    "* Check Descibe option similar to Pandas\n",
    "* Adding and Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df_pyspark = spark.read.option('header','true').csv('pysparktest.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Jayden|  23|         5| 80000|\n",
      "| Shawn|  27|         7| 65000|\n",
      "|   Bob|null|         4| 12345|\n",
      "|Jeremy|  40|         8| 69000|\n",
      "|Joseph|  23|         4|  null|\n",
      "|  Mary|  24|      null|100000|\n",
      "|  null|null|      null|  null|\n",
      "|  null|  34|         2| 40000|\n",
      "|  null| 121|      null| 90000|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('pysparktest.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| Age|\n",
      "+------+----+\n",
      "|Jayden|  23|\n",
      "| Shawn|  27|\n",
      "|   Bob|null|\n",
      "|Jeremy|  40|\n",
      "|Joseph|  23|\n",
      "|  Mary|  24|\n",
      "|  null|null|\n",
      "|  null|  34|\n",
      "|  null| 121|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting a column\n",
    "df_pyspark.select('Name', 'Age').show()\n",
    "# Notice it is like SQL, hence, pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the cloumn types\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+------------------+\n",
      "|summary| Name|               Age|        Experience|            Salary|\n",
      "+-------+-----+------------------+------------------+------------------+\n",
      "|  count|    6|                 7|                 6|                 7|\n",
      "|   mean| null|41.714285714285715|               5.0|65192.142857142855|\n",
      "| stddev| null| 35.54206093121353|2.1908902300206643|30244.257286458927|\n",
      "|    min|  Bob|                23|                 2|             12345|\n",
      "|    max|Shawn|               121|                 8|            100000|\n",
      "+-------+-----+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See simple stats of the dataframe\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Columns in a data frame\n",
    "df_pyspark= df_pyspark.withColumn('Experience after 2 years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+------------------------+\n",
      "|  Name| Age|Experience|Salary|Experience after 2 years|\n",
      "+------+----+----------+------+------------------------+\n",
      "|Jayden|  23|         5| 80000|                       7|\n",
      "| Shawn|  27|         7| 65000|                       9|\n",
      "|   Bob|null|         4| 12345|                       6|\n",
      "|Jeremy|  40|         8| 69000|                      10|\n",
      "|Joseph|  23|         4|  null|                       6|\n",
      "|  Mary|  24|      null|100000|                    null|\n",
      "|  null|null|      null|  null|                    null|\n",
      "|  null|  34|         2| 40000|                       4|\n",
      "|  null| 121|      null| 90000|                    null|\n",
      "+------+----+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns in a dataframe\n",
    "df_spark=df_pyspark.drop('Experience after 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[First Name: string, Age: int, Experience: int, Salary: int, Experience after 2 years: int]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the column\n",
    "df_pyspark.withColumnRenamed('Name','First Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Fiter Functions\n",
    "* Filter Operation\n",
    "* &, |, ==\n",
    "* ~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|Jayden|  23|         5| 80000|\n",
      "| Shawn|  27|         7| 65000|\n",
      "|   Bob|null|         4| 12345|\n",
      "|Jeremy|  40|         8| 69000|\n",
      "|Joseph|  23|         4|  null|\n",
      "|  Mary|  24|      null|100000|\n",
      "|  null|null|      null|  null|\n",
      "|  null|  34|         2| 40000|\n",
      "|  null| 121|      null| 90000|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark1=spark.read.csv('pysparktest.csv', header=True,inferSchema=True)\n",
    "df_spark1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int, Experience after 2 years: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salary of the people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary<=20000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|Name| Age|\n",
      "+----+----+\n",
      "| Bob|null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"Salary<=20000\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+------------------------+\n",
      "|  Name|Age|Experience|Salary|Experience after 2 years|\n",
      "+------+---+----------+------+------------------------+\n",
      "|Jayden| 23|         5| 80000|                       7|\n",
      "| Shawn| 27|         7| 65000|                       9|\n",
      "|Jeremy| 40|         8| 69000|                      10|\n",
      "|  null| 34|         2| 40000|                       4|\n",
      "+------+---+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More than one condition\n",
    "df_pyspark.filter((df_pyspark['Salary']<=80000) & \n",
    "    (df_pyspark['Salary']>=15000)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7a49970c4512c0722528751db23d666149f0368bc6c09b7f6ca5769874537f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
